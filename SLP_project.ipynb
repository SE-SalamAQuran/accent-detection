{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLP project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEh5eMTCjceSEngZ2wajFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SE-SalamAQuran/accent-detection/blob/master/SLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GvzK3lGeos6"
      },
      "source": [
        "**SLP accent detection project.**\n",
        "\n",
        "**Spring 2020/2021.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ilcawofHqe"
      },
      "source": [
        "*Prepared by:*\n",
        "\n",
        "*   Salam Quran: \"1161667\"\n",
        "*   Mohye Ahmad: \"1162843\"\n",
        "*   Sara Arar: \"1160880\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZYBIT7legOx"
      },
      "source": [
        "!pip install librosa\n",
        "!pip install python_speech_features\n",
        "!pip install kneed\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuI6QPE4h7mD"
      },
      "source": [
        "#Importing MatplotLib for plotting the signal graphs\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import statistics\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "import scipy as sp\n",
        "from __future__ import division\n",
        "import sys\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from kneed import KneeLocator\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib as mpl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmlLHlmjh9cL"
      },
      "source": [
        "from google.colab import drive  #Getting the data set from the drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh7kGI4Gig3X"
      },
      "source": [
        "Now we get the zipped file and unzip it to obtain the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1dyHm5nirr3"
      },
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive/\n",
        "!unzip /mydrive/Spoken/PS_Accents-20210604T093259Z-001.zip -d /content/sample_data/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIt-x3Pn2Vmb"
      },
      "source": [
        "Preprocessing & Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awA6VknP3DNX"
      },
      "source": [
        "%cd sample_data/PS_Accents/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgntKqtPo5oj"
      },
      "source": [
        "#Preprocessing & Feature Extraction\n",
        "\n",
        "def preprocessing(path):\n",
        "    #Read audio data from path\n",
        "    data, rate = librosa.load(path,offset=0.0)\n",
        "\n",
        "    \n",
        "    #Pre-emphasis\n",
        "    data = librosa.effects.preemphasis(data)\n",
        "\n",
        "\n",
        "    #Feature1: MFCC\n",
        "    X = librosa.feature.mfcc(y=data, sr=rate)\n",
        "    avg_mfcc = np.mean(X) \n",
        "    \n",
        "    #Feature2: Zero-crossings Rate\n",
        "    Y = librosa.feature.zero_crossing_rate(data)\n",
        "    avg_ZCR = np.mean(Y) \n",
        "\n",
        "    #Feature3: Spectral Roll_off\n",
        "    # Approximate maximum frequencies with roll_percent=0.85 (default)\n",
        "    Z = librosa.feature.spectral_rolloff(y=data, sr=rate)\n",
        "    avg_roll =  np.mean(Z) \n",
        "\n",
        "    #Feature4: Chroma Frequencies\n",
        "    W = librosa.feature.chroma_stft(data, rate)\n",
        "    avg_chroma = np.average(W) \n",
        "\n",
        "\n",
        "\n",
        "    return avg_mfcc, avg_ZCR, avg_roll, avg_chroma\n",
        "   \n",
        "warnings.filterwarnings('ignore')\n",
        "def plot_signal(title, x_label, y_label, data):\n",
        "        plt.figure()\n",
        "        plt.plot(data)\n",
        "        plt.xlabel(x_label)\n",
        "        plt.ylabel(y_label)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zFPJRhkjXbF"
      },
      "source": [
        "\n",
        "#Average Features for all Jerusalem dataset\n",
        "\n",
        "\n",
        "def getFeaturesJerusalem():\n",
        "  directory = 'Jerusalem/'\n",
        "  mfccs = []\n",
        "  zcrs = []\n",
        "  roll_offs = []\n",
        "  chroma_freqs = []\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = preprocessing(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "  # print(\"Jerusalem Accent Features\")  \n",
        "  # print(\"MAX MFCC: \", np.round(max(mfccs), 3))\n",
        "  # print(\"MIN MFCC: \", np.round(min(mfccs),3))\n",
        "  # print(\"MAX Zero Crossings Rate: \", np.round(max(zcrs),3))\n",
        "  # print(\"MIN Zero Crossings Rate: \", np.round(min(zcrs),3))  \n",
        "  # print(\"MAX Roll-Off Rate: \", np.round(max(roll_offs),3))\n",
        "  # print(\"MIN Roll-Off Rate: \", np.round(min(roll_offs),3))   \n",
        "  # print(\"MAX Chroma Frequency: \", np.round(max(chroma_freqs),3))\n",
        "  # print(\"MIN Chroma Frequency: \", np.round(min(chroma_freqs),3))   \n",
        "  # print(\"Average MFCC: \",round(sum(mfccs) / len(mfccs), 3))\n",
        "  # print(\"Average Zero Crossing Rate: \",round(sum(zcrs) / len(zcrs), 3))\n",
        "  # print(\"Average Roll-Off: \",round(sum(roll_offs) / len(roll_offs), 3))\n",
        "  # print(\"Average Chroma Frequency: \",round(sum(chroma_freqs) / len(chroma_freqs), 3))\n",
        "  return round(sum(mfccs) / len(mfccs), 3) , round(sum(zcrs) / len(zcrs), 3), round(sum(roll_offs) / len(roll_offs), 3), round(sum(chroma_freqs) / len(chroma_freqs), 3)\n",
        "\n",
        "print(\"\")\n",
        "#Plotting the results\n",
        "# plot_signal('Jerusalem MFCC Values', 'Time', '', mfccs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Jerusalem ZCR Values', 'Time', '', zcrs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Jerusalem Roll-off Values', 'Time', '', roll_offs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Jerusalem Chroma-Frequencies Values', 'Time', '', chroma_freqs)\n",
        "# print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTB1EfZ-xXl"
      },
      "source": [
        "\n",
        "#Average Features for all Jerusalem dataset\n",
        "\n",
        "\n",
        "def getFeaturesHebron():\n",
        "  directory = 'Hebron/'\n",
        "  mfccs = []\n",
        "  zcrs = []\n",
        "  roll_offs = []\n",
        "  chroma_freqs = []\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = preprocessing(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "  # print(\"Hebron Accent Features\")  \n",
        "  # print(\"MAX MFCC: \", np.round(max(mfccs), 3))\n",
        "  # print(\"MIN MFCC: \", np.round(min(mfccs),3))\n",
        "  # print(\"MAX Zero Crossings Rate: \", np.round(max(zcrs),3))\n",
        "  # print(\"MIN Zero Crossings Rate: \", np.round(min(zcrs),3))  \n",
        "  # print(\"MAX Roll-Off Rate: \", np.round(max(roll_offs),3))\n",
        "  # print(\"MIN Roll-Off Rate: \", np.round(min(roll_offs),3))   \n",
        "  # print(\"MAX Chroma Frequency: \", np.round(max(chroma_freqs),3))\n",
        "  # print(\"MIN Chroma Frequency: \", np.round(min(chroma_freqs),3))   \n",
        "  # print(\"Average MFCC: \",round(sum(mfccs) / len(mfccs), 3))\n",
        "  # print(\"Average Zero Crossing Rate: \",round(sum(zcrs) / len(zcrs), 3))\n",
        "  # print(\"Average Roll-Off: \",round(sum(roll_offs) / len(roll_offs), 3))\n",
        "  # print(\"Average Chroma Frequency: \",round(sum(chroma_freqs) / len(chroma_freqs), 3))\n",
        "  return round(sum(mfccs) / len(mfccs), 3) , round(sum(zcrs) / len(zcrs), 3), round(sum(roll_offs) / len(roll_offs), 3), round(sum(chroma_freqs) / len(chroma_freqs), 3)\n",
        "\n",
        "print(\"\")\n",
        "#Plotting the results\n",
        "# plot_signal('Hebron MFCC Values', 'Time', '', mfccs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Hebron ZCR Values', 'Time', '', zcrs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Hebron Roll-off Values', 'Time', '', roll_offs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Hebron Chroma-Frequencies Values', 'Time', '', chroma_freqs)\n",
        "# print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGJ1Klxu-_d-"
      },
      "source": [
        "\n",
        "def getFeaturesNablus():\n",
        "  directory = 'Nablus/'\n",
        "  mfccs = []\n",
        "  zcrs = []\n",
        "  roll_offs = []\n",
        "  chroma_freqs = []\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = preprocessing(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "  print(\"Nablus Accent Features\")  \n",
        "  # print(\"MAX MFCC: \", np.round(max(mfccs), 3))\n",
        "  # print(\"MIN MFCC: \", np.round(min(mfccs),3))\n",
        "  # print(\"MAX Zero Crossings Rate: \", np.round(max(zcrs),3))\n",
        "  # print(\"MIN Zero Crossings Rate: \", np.round(min(zcrs),3))  \n",
        "  # print(\"MAX Roll-Off Rate: \", np.round(max(roll_offs),3))\n",
        "  # print(\"MIN Roll-Off Rate: \", np.round(min(roll_offs),3))   \n",
        "  # print(\"MAX Chroma Frequency: \", np.round(max(chroma_freqs),3))\n",
        "  # print(\"MIN Chroma Frequency: \", np.round(min(chroma_freqs),3))   \n",
        "  # print(\"Average MFCC: \",round(sum(mfccs) / len(mfccs), 3))\n",
        "  # print(\"Average Zero Crossing Rate: \",round(sum(zcrs) / len(zcrs), 3))\n",
        "  # print(\"Average Roll-Off: \",round(sum(roll_offs) / len(roll_offs), 3))\n",
        "  # print(\"Average Chroma Frequency: \",round(sum(chroma_freqs) / len(chroma_freqs), 3))\n",
        "  return round(sum(mfccs) / len(mfccs), 3) , round(sum(zcrs) / len(zcrs), 3), round(sum(roll_offs) / len(roll_offs), 3), round(sum(chroma_freqs) / len(chroma_freqs), 3)\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "#Plotting the result\n",
        "# plot_signal('Nablus MFCC Values', 'Time', '', mfccs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Nablus ZCR Values', 'Time', '', zcrs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Nablus Roll-off Values', 'Time', '', roll_offs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Nablus Chroma-Frequencies Values', 'Time', '', chroma_freqs)\n",
        "# print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6wk5qx_ArG8"
      },
      "source": [
        "\n",
        "def getFeaturesRamallah(directory):\n",
        "  directory = 'Ramallah_Reef/'\n",
        "  mfccs = []\n",
        "  zcrs = []\n",
        "  roll_offs = []\n",
        "  chroma_freqs = []\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = preprocessing(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "  # print(\"Ramallah-Reef Accent Features\")  \n",
        "  # print(\"MAX MFCC: \", np.round(max(mfccs), 3))\n",
        "  # print(\"MIN MFCC: \", np.round(min(mfccs),3))\n",
        "  # print(\"MAX Zero Crossings Rate: \", np.round(max(zcrs),3))\n",
        "  # print(\"MIN Zero Crossings Rate: \", np.round(min(zcrs),3))  \n",
        "  # print(\"MAX Roll-Off Rate: \", np.round(max(roll_offs),3))\n",
        "  # print(\"MIN Roll-Off Rate: \", np.round(min(roll_offs),3))   \n",
        "  # print(\"MAX Chroma Frequency: \", np.round(max(chroma_freqs),3))\n",
        "  # print(\"MIN Chroma Frequency: \", np.round(min(chroma_freqs),3))   \n",
        "  # print(\"Average MFCC: \",round(sum(mfccs) / len(mfccs), 3))\n",
        "  # print(\"Average Zero Crossing Rate: \",round(sum(zcrs) / len(zcrs), 3))\n",
        "  # print(\"Average Roll-Off: \",round(sum(roll_offs) / len(roll_offs), 3))\n",
        "  # print(\"Average Chroma Frequency: \",round(sum(chroma_freqs) / len(chroma_freqs), 3))\n",
        "  return round(sum(mfccs) / len(mfccs), 3) , round(sum(zcrs) / len(zcrs), 3), round(sum(roll_offs) / len(roll_offs), 3), round(sum(chroma_freqs) / len(chroma_freqs), 3)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "#Plotting the result\n",
        "# plot_signal('Ramallah MFCC Values', 'Time', '', mfccs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Ramallah ZCR Values', 'Time', '', zcrs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Ramallah Roll-off Values', 'Time', '', roll_offs)\n",
        "# print(\"\")\n",
        "\n",
        "# plot_signal('Ramallah Chroma-Frequencies Values', 'Time', '', chroma_freqs)\n",
        "# print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3VLML7tHa6O"
      },
      "source": [
        "#Read Labeled data and feed it to the model \n",
        "#We're going to use K-means clustering and calculate Minimum Euclidean Distance\n",
        "\n",
        "def trainOnJerusalem(path):\n",
        "  avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs  = getFeaturesJerusalem()\n",
        "  features = [avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs]\n",
        "  mfcc, zcr, roll_off, chroma_freqs = preprocessing(path=path)\n",
        "\n",
        "  \n",
        "  mfcc_distance = sp.spatial.distance.euclidean(mfcc, avg_mfcc)\n",
        "  zcr_distance = sp.spatial.distance.euclidean(zcr, avg_zcr)\n",
        "  rollOff_distance = sp.spatial.distance.euclidean(roll_off, avg_roll_off)\n",
        "  chromaFreq_distance = sp.spatial.distance.euclidean(chroma_freqs, avg_chroma_freqs)\n",
        "  return mfcc_distance, zcr_distance, rollOff_distance, chromaFreq_distance\n",
        "\n",
        "  #...............................................................................\n",
        "directory = 'Jerusalem/'\n",
        "mfccs = []\n",
        "zcrs = []\n",
        "roll_offs = []\n",
        "chroma_freqs = []\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = trainOnJerusalem(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "print(\"Euclidean Distance of MFCCs:\", X)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of ZCRs:\", Y)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Roll_Offs:\", Z)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Chormas:\", W)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSo-wwTn-l00"
      },
      "source": [
        "#Read Labeled data and feed it to the model \n",
        "#We're going to use K-means clustering and calculate Minimum Euclidean Distance\n",
        "\n",
        "def trainOnHebron(path):\n",
        "  avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs  = getFeaturesJerusalem()\n",
        "  features = [avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs]\n",
        "  mfcc, zcr, roll_off, chroma_freqs = preprocessing(path=path)\n",
        "\n",
        "  \n",
        "  mfcc_distance = sp.spatial.distance.euclidean(mfcc, avg_mfcc)\n",
        "  zcr_distance = sp.spatial.distance.euclidean(zcr, avg_zcr)\n",
        "  rollOff_distance = sp.spatial.distance.euclidean(roll_off, avg_roll_off)\n",
        "  chromaFreq_distance = sp.spatial.distance.euclidean(chroma_freqs, avg_chroma_freqs)\n",
        "  return mfcc_distance, zcr_distance, rollOff_distance, chromaFreq_distance\n",
        "\n",
        "  #...............................................................................\n",
        "directory = 'Hebron/'\n",
        "mfccs = []\n",
        "zcrs = []\n",
        "roll_offs = []\n",
        "chroma_freqs = []\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = trainOnHebron(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "print(\"Euclidean Distance of MFCCs:\", X)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of ZCRs:\", Y)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Roll_Offs:\", Z)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Chormas:\", W)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWh3BsOA-_6H"
      },
      "source": [
        "#Read Labeled data and feed it to the model \n",
        "#We're going to use K-means clustering and calculate Minimum Euclidean Distance\n",
        "\n",
        "def trainOnNablus(path):\n",
        "  avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs  = getFeaturesJerusalem()\n",
        "  features = [avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs]\n",
        "  mfcc, zcr, roll_off, chroma_freqs = preprocessing(path=path)\n",
        "\n",
        "  \n",
        "  mfcc_distance = sp.spatial.distance.euclidean(mfcc, avg_mfcc)\n",
        "  zcr_distance = sp.spatial.distance.euclidean(zcr, avg_zcr)\n",
        "  rollOff_distance = sp.spatial.distance.euclidean(roll_off, avg_roll_off)\n",
        "  chromaFreq_distance = sp.spatial.distance.euclidean(chroma_freqs, avg_chroma_freqs)\n",
        "  return mfcc_distance, zcr_distance, rollOff_distance, chromaFreq_distance\n",
        "\n",
        "  #...............................................................................\n",
        "directory = 'Nablus/'\n",
        "mfccs = []\n",
        "zcrs = []\n",
        "roll_offs = []\n",
        "chroma_freqs = []\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = trainOnNablus(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "print(\"Euclidean Distance of MFCCs:\", X)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of ZCRs:\", Y)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Roll_Offs:\", Z)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Chormas:\", W)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfYRK9ho_HUz"
      },
      "source": [
        "#Read Labeled data and feed it to the model \n",
        "#We're going to use K-means clustering and calculate Minimum Euclidean Distance\n",
        "\n",
        "def trainOnRamallah(path):\n",
        "  avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs  = getFeaturesJerusalem()\n",
        "  features = [avg_mfcc, avg_zcr, avg_roll_off, avg_chroma_freqs]\n",
        "  mfcc, zcr, roll_off, chroma_freqs = preprocessing(path=path)\n",
        "\n",
        "  \n",
        "  mfcc_distance = sp.spatial.distance.euclidean(mfcc, avg_mfcc)\n",
        "  zcr_distance = sp.spatial.distance.euclidean(zcr, avg_zcr)\n",
        "  rollOff_distance = sp.spatial.distance.euclidean(roll_off, avg_roll_off)\n",
        "  chromaFreq_distance = sp.spatial.distance.euclidean(chroma_freqs, avg_chroma_freqs)\n",
        "  return mfcc_distance, zcr_distance, rollOff_distance, chromaFreq_distance\n",
        "\n",
        "  #...............................................................................\n",
        "directory = 'Ramallah_Reef/'\n",
        "mfccs = []\n",
        "zcrs = []\n",
        "roll_offs = []\n",
        "chroma_freqs = []\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".wav\"):\n",
        "      X, Y, Z, W = trainOnRamallah(os.path.join(directory, filename))\n",
        "      mfccs.append(X)\n",
        "      zcrs.append(Y)\n",
        "      roll_offs.append(Z)\n",
        "      chroma_freqs.append(W)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "print(\"Euclidean Distance of MFCCs:\", X)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of ZCRs:\", Y)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Roll_Offs:\", Z)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Euclidean Distance of Chormas:\", W)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}